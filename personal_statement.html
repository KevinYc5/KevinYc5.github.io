<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc_letter.css" type="text/css" />
<link rel="stylesheet" href="jacob.css" type="text/css" />
<link rel="stylesheet" href="person.css" type="text/css" />
<title>Kevin Xue (薛凯文)</title>
</head>
<body>
<div id="layout-content">

<table class="imgtable"><tr><td>
<img src="picture/William_n_lipscomb_jr.jpg"  alt="alt text" width="150px" />&nbsp;</td>
<td align="left"><i>“With a Ph.D. you will have a better chance of spending the rest of your life doing what you want to do, instead of what someone else wants you to do.” </i><br />
    <p style="text-align: right" >- William Lipscomb</p>
</td></tr></table>
<div id="toptitle">
<h1>Personal statement</h1>
</div>
<p>Nobel Laureate in Chemistry William Lipscomb once said: “With a Ph.D. you will have a better chance of spending the rest of your life doing what you want to do,
    instead of what someone else wants you to do.” I couldn’t agree more, because I prefer to research and develop the most advanced technologies than to
    study how to implement "obsolete" technologies. With a PhD I am more likely to get interesting and challenging jobs in academia or industry.
    By pursuing a Ph.D., I believe I can gain the ability to achieve these goals.</p>

    <p>My undergraduate experience can be divided into the following three parts, which have given me self-discipline, resilience, skills in communication and planning,
        and prepared me for research.</p>
<ul>
    <li>
        <b>Excellent grades</b>. No step is too small, and I have not slacked in my basic mathematics studies and programming training which bring me
        high ranking of <b>2/179</b> in my first five semesters. I have achieved excellent grades in several courses such as Data Structures (99), Probability Theory (95),
        in addition to self-study in statistical learning methods, Andrew NG: Deep learning, Stanford CS231n: Deep learning for Computer Vision,
        Stanford CS236: Deep Generative model and other machine learning related courses.
    </li>
    <li>
        <b>Strong mathematical modeling skills</b>. I am obsessed with using mathematical models to solve real-world problems.
        Studying mathematical modeling theory and methods, I have gained an understanding of optimization models, dynamic models and probabilistic models.
        I won awards such as the first prize in the Sichuan Region of the National Student Mathematical Modeling Competition 2021.
    </li>
    <li><p>
        <b>Experienced a complete training in scientific research</b>. I Joined Associate Professor Fan Zhou's team in the first semester of my junior year.
        Recently, many literature reveals that diffusion probabilistic models can be used to synthesize high-fidelity samples with competitive
        or sometimes better quality than previous state-of-the-art approaches and we aim to apply the DDPM to speech enhancement tasks.
        In this work, we first reveal the difficulties in directly applying existing conditional diffusion models to the field of speech enhancement such as
        problems of lack of structural guarantee and low model convergence rate brought by the black box models.
        Then we introduce Prior-DiffuSE, a simple and effective framework for speech enhancement using conditional diffusion models.
        Inspired by predict-and-refine strategy, we adopted the approach of introducing a discriminative auxiliary model and
        re-derived the conditional probability formula. We made a theoretical analysis of the model convergence rate under the assumption that the model is linear.
        As a result, our proposed method achieves state-of-the-art performance against GAN-based model and shows a significant improvement over existing DDPM-based
        algorithms. In the future, we will apply our model to the image-synthesize tasks and compare it with the interpolation method
        In addition, under the guidance of Associate
        Professor Deyuan Chen at the University of Chinese Academy of Sciences, I am researching the topic of object detection of dense crowds in point cloud data.
    </p></li>
</ul>
<p>
    The famous physicist Feynman wrote this sentence on the blackboard on the day he left this life:
    <i>“What I cannot create, I do not understand.”</i>, which implies the measure of thorough understanding. The deep generative model
    is the method that emerged from this idea. The relationship between deep generative models and common discriminative models can
    be analogous to the relationship between joint probability and conditional probability in probability statistics, and
    a discriminative model can be considered as a simple conditional generative model. Generative models can be applied not only
    to common discriminative tasks, but also to some challenging inverse problems and interesting generative tasks. Generative models
    usually face three challenges: representation, learning and inference, which are coupled with each other. In order to generate high quality and diversity of samples,
    accurate probability density estimates, and to obtain higher-order semantic features, Different models were proposed with different abilities of representation,
    learning and inference, including fully observed likelihood-based models such as autoregressive models and flow-based models; latent variable models such as variational autoencoders;
    implicit generative models such as GAN; energy-based models and score-based models. Fully observed likelihood-based models refer to
    those models that can obtain tractable likelihood functions that can be estimated by MLE.
    For example, autoregressive models introduce conditional independence through a probability graph model and increase the
    expressiveness of the model by using neural networks to parameterize the conditional distribution function, which
    eventually yields a compact representation of the target distribution and facilitates the calculation of the likelihood function
    and the estimation of the maximum likelihood. However, higher log-likelihood doesn’t necessarily mean better looking samples.
    The latent variable model supports unsupervised learning by introducing latent variables
    and defines the target distribution representation flexibly leading to difficulty in maximum likelihood estimation, and usually
    uses the technique of variational inference for optimization, which cannot obtain the exact likelihood.
    The implicit generative model GAN is more concerned with the quality of the generated samples and learned through a
    two-sample test objective function, where the generator is used to minimize the distance from the target distribution
    and the discriminator is used to tighten the lower bound of the function. These models have their own pros and cons
    , and how to solve their respective pain points is what needs to be studied in the future.
    Furthermore, how to optimize the generative model better from theoretical perspective and ensure its generalization ability are important research direction.
    In addition to the research on deep generative modeling theory, the
    Applied research on generative models includes but is not limited to image, text, speech, text-to-image, and even medical drug molecule generation.
</p>
<p>
  From a semi-supervised learning perspective, generative modeling frameworks provide an approach for semi-supervised learning,
  such as semi-supervised Gaussian mixture models that integrate the ideas of Bayesian classifiers and Gaussian mixture models, 
  effectively exploiting the information of the implied distribution of unlabeled sample data. In addition to simple Gaussian mixture models,
  hidden variable models such as VAE have good properties to integrate supervised and unsupervised learning.
</p>
<p>
  Finally, with the explosion of deep learning based on "heuristics", are we concerned with the essence of the problem:
  how to find information from low-rank, sparse data. To solve this problem I think we need to master the necessary mathematical tools,
  I read David C. Lay's Linear Algebra and its Applications and Frank R. Giordano's Mathematical Modeling, which provide model-based methodologies
  for describing and solving problems such as compressed perception algorithms in computer vision. On this basis, I believe a better understanding 
  of deep learning is possible.
    To sum up, I want to get my Ph.D. in order to pursue an interesting and challenging career in
    academia or industry. My undergraduate experience has taught me the self-discipline and tenacity that will give me the confidence to
    make my own contributions to the field of computer science in the future.
</p>
<p style="text-align: center">
    <img src="picture/feynman.jpeg"  alt="feynman" style="" width="500px" />&nbsp;</p>
    <p style="text-align: center">
        feynman's final blackboard&nbsp;</p>
<div id="footer">
<div id="footer-text">
<br>Page generated 2022-05-27, by <a href="https://kevinyc5.github.io/">Kaiwen Xue</a>.
</div>
</div>
</div>
</body>
</html>
